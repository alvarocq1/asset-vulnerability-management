import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, make_scorer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import joblib

# Cargar los datos
data = pd.read_csv('cves_actualizados.csv')

# Función para extraer y mapear los vectores CVSS a sus componentes
def extract_cvss_components(cvss_vector):
    if pd.isna(cvss_vector):
        return {}
    components = cvss_vector.split('/')
    components_dict = {}
    for comp in components:
        key, value = comp.split(':')
        components_dict[key] = value
    return components_dict

# Aplicar el mapeo a los vectores CVSS
data['CVSS 2.0 Components'] = data['CVSS 2.0 Vector'].apply(extract_cvss_components)
data['CVSS 3.1 Components'] = data['CVSS 3.1 Vector'].apply(extract_cvss_components)

# Convertir los componentes mapeados a DataFrame
cvss_2_0_df = pd.DataFrame(data['CVSS 2.0 Components'].tolist())
cvss_3_1_df = pd.DataFrame(data['CVSS 3.1 Components'].tolist())
cvss_3_1_df = cvss_3_1_df.drop(columns=['CVSS'])

# Filtrar los datos para asegurar que no haya valores nulos
data_filtered = data[(cvss_2_0_df.notnull().all(axis=1)) & (cvss_3_1_df.notnull().all(axis=1))]
cvss_2_0_df = cvss_2_0_df.loc[data_filtered.index]
cvss_3_1_df = cvss_3_1_df.loc[data_filtered.index]

print(cvss_2_0_df.head())
print(cvss_3_1_df.head())

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(cvss_2_0_df, cvss_3_1_df, test_size=0.2, random_state=42)

# Verificar si hay datos nulos en los conjuntos de entrenamiento y prueba
print("X_train nulos:", X_train.isnull().sum().sum())
print("y_train nulos:", y_train.isnull().sum().sum())
print("X_test nulos:", X_test.isnull().sum().sum())
print("y_test nulos:", y_test.isnull().sum().sum())

# Codificar las características categóricas de X usando OneHotEncoder
column_transformer = ColumnTransformer(
    transformers=[
        ('encoder', OneHotEncoder(), X_train.columns)
    ],
    remainder='passthrough'
)

# Ajustar el ColumnTransformer con los datos de entrenamiento
column_transformer.fit(X_train)

# Transformar los datos de entrenamiento y prueba
X_train_transformed = column_transformer.transform(X_train)
X_test_transformed = column_transformer.transform(X_test)

# Definir una función de scoring personalizada para GridSearchCV
scorer = make_scorer(accuracy_score)

# Definir los parámetros para GridSearchCV para kNN
params = {'n_neighbors': [5, 10, 20], 'weights': ['uniform', 'distance']}

best_models = {}
label_encoders = {}
test_accuracies = []

# Entrenar y evaluar el modelo kNN usando GridSearchCV para cada columna de y_train
columns = cvss_3_1_df.columns

for col in columns:
    y_train_col = y_train[col]
    y_test_col = y_test[col]

    # Convertir las etiquetas a categóricas
    le = LabelEncoder()
    y_train_col_encoded = le.fit_transform(y_train_col)
    y_test_col_encoded = le.transform(y_test_col)

    label_encoders[col] = le

    print(f"Training kNN for target column {col}...")
    
    # Crear el pipeline con el modelo
    pipeline = Pipeline(steps=[
        ('classifier', KNeighborsClassifier())
    ])
    
    grid = GridSearchCV(pipeline, param_grid={'classifier__n_neighbors': [5, 10, 20], 'classifier__weights': ['uniform', 'distance']}, cv=10, scoring=scorer, n_jobs=7)
    grid.fit(X_train_transformed, y_train_col_encoded)
    
    best_models[f"kNN_col_{col}"] = grid.best_estimator_
    print(f"Best params for kNN (target column {col}): {grid.best_params_}")
    print(f"Best score for kNN (target column {col}): {grid.best_score_}")

    # Evaluar el modelo en el conjunto de prueba
    y_test_pred = grid.predict(X_test_transformed)
    test_accuracy = accuracy_score(y_test_col_encoded, y_test_pred)
    print(f"Test set accuracy for {col}: {test_accuracy:.4f}")

    # Almacenar las precisiones del conjunto de prueba
    test_accuracies.append(test_accuracy)

# Guardar los modelos, los codificadores de etiquetas y el ColumnTransformer
joblib.dump((best_models, label_encoders, column_transformer), 'best_models_knn_classification.pkl')

# Calcular el porcentaje de acierto general
average_accuracy = sum(test_accuracies) / len(test_accuracies) * 100
print(f"Porcentaje de acierto general en el conjunto de prueba: {average_accuracy:.2f}%")
